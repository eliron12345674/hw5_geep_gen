{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "import random\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "DATASET_PATH = \"../data\"\n",
    "savedModels_PATH = \"finalProduct2\"\n",
    "lossGraph_PATH = \"finalGraph\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "def add_uniform_noise(sample):\n",
    "    return sample.float() + torch.rand_like(sample.float())\n",
    "\n",
    "def scale_to_unit_interval(sample):\n",
    "    return sample / 255.0\n",
    "cuda = True\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "x_dim  = 784\n",
    "hidden_dim = 128\n",
    "latent_dim = 64\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "static_var = 0.1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "add_uniform_noise,\n",
    "scale_to_unit_interval\n",
    "])\n",
    "\n",
    "train_dataset = MNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [50000, 10000])\n",
    "test_set = MNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True)\n",
    "val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False)\n",
    "test_loader = data.DataLoader(test_set, batch_size=16, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "elbo funciton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_elbo(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = torch.sum((x - x_hat)**2 / (2 * static_var**2) + torch.log(static_var * torch.sqrt(torch.tensor(2 * torch.pi))), dim=1)\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp(), dim=1)\n",
    "    \n",
    "    return torch.mean(reproduction_loss + KLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_conv_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, 1, kernel_size=3, stride=2, padding=1) for _ in range(num_conv_layers)])\n",
    "\n",
    "        # Create a list to store convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 1, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Adjust input_dim to account for the concatenated outputs\n",
    "        self.FC_input = nn.Linear(input_dim // 4 * num_conv_layers, hidden_dim)\n",
    "        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.FC_var   = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.training = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Reshape input\n",
    "        x = x.view(batch_size, 1, 28, 28)\n",
    "        \n",
    "        # Process input through each convolutional layer\n",
    "        x = self.LeakyReLU(self.conv1(x))\n",
    "        x = self.LeakyReLU(self.conv2(x))\n",
    "        \n",
    "        # Concatenate outputs along the channel dimension\n",
    "        \n",
    "        # Flatten the concatenated output\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.LeakyReLU(self.FC_input(x))\n",
    "        mean = self.FC_mean(x)\n",
    "        log_var = self.FC_var(x)\n",
    "                                                    \n",
    "        return mean, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim, num_conv_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        \n",
    "        # Adjusting for the input size to the deconvolution layers\n",
    "        deconv_input_dim = (output_dim // 4) * num_conv_layers\n",
    "        self.FC_output = nn.Linear(hidden_dim, deconv_input_dim)\n",
    "        \n",
    "        \n",
    "        self.deconvs = nn.ModuleList()\n",
    "        for _ in range(num_conv_layers):\n",
    "            self.deconvs.append(nn.ConvTranspose2d(num_conv_layers, 1, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "        self.deconv =  nn.ConvTranspose2d(num_conv_layers, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        # Final deconv layer to get to original input size\n",
    "        self.deconv1 = nn.ConvTranspose2d(1, 1, kernel_size=3, stride=1, padding=1, output_padding=0)\n",
    "        self.deconv2 = nn.ConvTranspose2d(1, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.LeakyReLU(self.FC_hidden(x))\n",
    "        \n",
    "        x = self.LeakyReLU(self.FC_output(x))\n",
    "        \n",
    "        # Reshape to the correct dimensions for deconvolution\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1, 14, 14)  # Assuming the final conv layer outputs 7x7 feature maps\n",
    "        \n",
    "        # Process through each deconvolutional layer\n",
    "        # conv_outputs = [(F.relu(conv(h))) for conv in self.deconvs]\n",
    "        # x = torch.cat(conv_outputs, dim=1)\n",
    "        x = self.LeakyReLU(self.deconv1(x))\n",
    "        x = self.LeakyReLU(self.deconv2(x))\n",
    "        # Flatten the concatenated output\n",
    "        x = x.view(batch_size, -1)\n",
    "    \n",
    "        x = x.view(batch_size, 28, 28) \n",
    "        # Final deconvolution to match the original input dimensions\n",
    "        x_hat = torch.sigmoid(x)\n",
    "        x_hat = x_hat.view(batch_size, 784)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.Encoder = Encoder\n",
    "        self.Decoder = Decoder\n",
    "        \n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(DEVICE)        # sampling epsilon        \n",
    "        z = mean + var*epsilon                          # reparameterization trick\n",
    "        return z\n",
    "        \n",
    "                \n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.Encoder(x)\n",
    "        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -> var)\n",
    "        x_hat = self.Decoder(z)\n",
    "        \n",
    "        return x_hat, mean, log_var\n",
    "    \n",
    "\n",
    "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim, num_conv_layers=1)\n",
    "decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim, num_conv_layers=1)\n",
    "\n",
    "model = Model(Encoder=encoder, Decoder=decoder).to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    model.load_state_dict(torch.load(savedModels_PATH))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x200 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sample(number_of_images = 1): \n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(number_of_images, latent_dim).to(DEVICE)\n",
    "        generated_images = decoder(noise)\n",
    "        generated_images = generated_images.to('cpu')\n",
    "        imgs_reshaped = generated_images.view(number_of_images, 28, 28)\n",
    "        \n",
    "        # Set up the plot\n",
    "        fig, axes = plt.subplots(1, number_of_images, figsize=(number_of_images * 2, 2))\n",
    "\n",
    "        # Plot each image\n",
    "        for i in range(number_of_images):\n",
    "            axes[i].imshow(imgs_reshaped[i], cmap='gray')\n",
    "            axes[i].axis('off')  # Hide axes for better visualization\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "sample(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforms from 7 to 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_images(tensor):\n",
    "    num_images, squared_size = tensor.shape\n",
    "    img_size = int(squared_size**0.5)\n",
    "\n",
    "    # Reshape each image to its 2D shape\n",
    "    images = tensor.view(num_images, img_size, img_size)\n",
    "    images = images.detach().numpy()\n",
    "    # Plot all images\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 2, 2))\n",
    "    for i in range(num_images):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(images[i], cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "image1 = train_set[1] # 7\n",
    "image2 = train_set[5] # 2\n",
    "\n",
    "def transformsDigits(image1, image2):\n",
    "    model.eval()\n",
    "    print(f\"transforms from {image1[1]} to {image2[1]}\")\n",
    "    image1 = image1[0].to(DEVICE)\n",
    "    image2 = image2[0].to(DEVICE)\n",
    "    z1 = model.Encoder(image1)[0][0]\n",
    "    z2 = model.Encoder(image2)[0][0]\n",
    "    path = torch.stack([z1 + (i / (9)) * (z2 - z1) for i in range(10)])\n",
    "    images = model.Decoder(path)\n",
    "    visualize_images(images.cpu())\n",
    "\n",
    "plt.close('all')\n",
    "transformsDigits(image1, image2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytroch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
